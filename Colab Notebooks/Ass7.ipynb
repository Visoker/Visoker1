{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ass7.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPsC+qrdhPyl0oPJDvgPaCV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"cXi6SLvmn4ad","executionInfo":{"status":"ok","timestamp":1650537167231,"user_tz":-330,"elapsed":666,"user":{"displayName":"Prem Khandelwal","userId":"11239718280451340504"}},"outputId":"27a00a58-8e8a-4b72-8b90-798371975642"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}],"source":["import numpy as np\n","import nltk\n","punkt = nltk.download('punkt')\n","stopwords = nltk.download('stopwords')\n","wordnet = nltk.download('wordnet')\n","averaged_perceptron_tagger = nltk.download('averaged_perceptron_tagger')\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.tokenize import sent_tokenize"]},{"cell_type":"code","source":["text=  \"Lorem ipsum dolor sit amet. Delectus quasi est excepturi accusamus non culpa ullam qui aliquam cum voluptates galisum rem dignissimos autem.\""],"metadata":{"id":"oiO2Lh9IowIm","executionInfo":{"status":"ok","timestamp":1650537167750,"user_tz":-330,"elapsed":21,"user":{"displayName":"Prem Khandelwal","userId":"11239718280451340504"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["tokenized_text = sent_tokenize(text)\n","print(tokenized_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HLL_EFXspDfe","executionInfo":{"status":"ok","timestamp":1650537167751,"user_tz":-330,"elapsed":20,"user":{"displayName":"Prem Khandelwal","userId":"11239718280451340504"}},"outputId":"36ff9577-8a19-4914-8b1c-7fe3470c0344"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["['Lorem ipsum dolor sit amet.', 'Delectus quasi est excepturi accusamus non culpa ullam qui aliquam cum voluptates galisum rem dignissimos autem.']\n"]}]},{"cell_type":"code","source":["stop_words = set(stopwords.words(\"english\"))\n","print(stop_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ftWNgd4ZpZhg","executionInfo":{"status":"ok","timestamp":1650537167751,"user_tz":-330,"elapsed":16,"user":{"displayName":"Prem Khandelwal","userId":"11239718280451340504"}},"outputId":"2a5e4e06-6b39-4a88-8d59-452d95ce4fe7"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["{'on', 'were', 'any', 'aren', 'once', 'and', 't', 'out', 'further', 'until', \"should've\", \"hadn't\", 'i', 'don', 'shan', \"don't\", \"you'll\", 'here', 'other', 'it', 'having', 'where', 'such', 'nor', 'but', 'haven', 'we', 'in', 'few', 'mustn', \"won't\", 'her', 'herself', 'needn', 'weren', 'more', 'is', \"hasn't\", 'was', \"couldn't\", \"it's\", 'about', 'are', 'hadn', 'will', 'shouldn', 'some', 'yours', 'this', 'been', 'did', 'she', 'or', \"you'd\", 'if', 've', 'by', 'all', 'an', 'can', 'while', 'there', 'theirs', 'me', 'ain', 'most', 'that', \"shan't\", 'doing', 'each', 'then', \"shouldn't\", 're', 'm', \"mustn't\", 'you', \"aren't\", 'their', 'against', 'so', 'off', 'of', 'mightn', 'through', 'a', \"wouldn't\", 'now', 'not', \"didn't\", 'hers', 'above', 'his', 'than', 'being', 's', 'during', 'he', \"she's\", 'am', 'didn', \"you've\", \"you're\", 'over', 'do', 'your', 'him', 'our', 'ma', 'very', 'both', 'hasn', 'won', 'doesn', 'ourselves', 'the', 'whom', \"that'll\", 'at', 'yourself', 'myself', 'to', 'll', \"haven't\", 'they', 'own', 'as', 'only', 'after', 'its', 'too', \"wasn't\", 'my', 'from', 'for', 'these', 'between', 'why', 'had', 'ours', 'what', 'isn', 'itself', 'wasn', \"isn't\", 'down', 'does', 'just', 'because', 'which', 'up', 'has', 'under', 'be', 'who', 'y', 'again', 'no', \"mightn't\", 'with', 'before', 'below', 'couldn', 'how', 'yourselves', 'same', 'd', 'have', \"doesn't\", \"weren't\", 'those', \"needn't\", 'himself', 'wouldn', 'themselves', 'them', 'into', 'should', 'o', 'when'}\n"]}]},{"cell_type":"code","source":["tokens= word_tokenize(text.lower())\n","filtered_text = []\n","for w in tokens:\n","  if w not in stop_words:\n","    filtered_text.append(w)\n","print(\"TokenizedSentence:\", tokens)\n","print(\"FilterdSentence:\", filtered_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B2hNLd35pZtV","executionInfo":{"status":"ok","timestamp":1650537167752,"user_tz":-330,"elapsed":14,"user":{"displayName":"Prem Khandelwal","userId":"11239718280451340504"}},"outputId":"be3a4aa7-4196-4e9a-d512-fcc958c72a42"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["TokenizedSentence: ['lorem', 'ipsum', 'dolor', 'sit', 'amet', '.', 'delectus', 'quasi', 'est', 'excepturi', 'accusamus', 'non', 'culpa', 'ullam', 'qui', 'aliquam', 'cum', 'voluptates', 'galisum', 'rem', 'dignissimos', 'autem', '.']\n","FilterdSentence: ['lorem', 'ipsum', 'dolor', 'sit', 'amet', '.', 'delectus', 'quasi', 'est', 'excepturi', 'accusamus', 'non', 'culpa', 'ullam', 'qui', 'aliquam', 'cum', 'voluptates', 'galisum', 'rem', 'dignissimos', 'autem', '.']\n"]}]},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","e_words = [\"wait\",\"waiting\",\"waited\",\"waits\"]\n","ps = PorterStemmer()\n","for w in e_words:\n","  rootWord = ps.stem(w)\n","  print(rootWord)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ShLla4D9rYIY","executionInfo":{"status":"ok","timestamp":1650537254627,"user_tz":-330,"elapsed":513,"user":{"displayName":"Prem Khandelwal","userId":"11239718280451340504"}},"outputId":"32673627-5a3d-46e0-ac75-1140c61ddda5"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["wait\n","wait\n","wait\n","wait\n"]}]},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","wordnet_lemmatizer = WordNetLemmatizer()\n","text = \"studies studying cries cry\"\n","tokenization = nltk.word_tokenize(text)\n","for w in tokenization:\n","  print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NAv-bZXKrlng","executionInfo":{"status":"ok","timestamp":1650537330817,"user_tz":-330,"elapsed":472,"user":{"displayName":"Prem Khandelwal","userId":"11239718280451340504"}},"outputId":"b19130e4-e338-4b7b-f06d-4c337c9f90f5"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Lemma for studies is study\n","Lemma for studying is studying\n","Lemma for cries is cry\n","Lemma for cry is cry\n"]}]},{"cell_type":"code","source":["import nltk \n","from nltk.tokenize import word_tokenize\n","data = \"The pink sweater fit her perfectly\"\n","words = word_tokenize(data)\n","for word in words:\n","  print(nltk.pos_tag([word]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8NrGs84NsTi0","executionInfo":{"status":"ok","timestamp":1650537397187,"user_tz":-330,"elapsed":486,"user":{"displayName":"Prem Khandelwal","userId":"11239718280451340504"}},"outputId":"039c0a38-4a5b-4bef-c73f-dc58c28308e5"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["[('The', 'DT')]\n","[('pink', 'NN')]\n","[('sweater', 'NN')]\n","[('fit', 'NN')]\n","[('her', 'PRP$')]\n","[('perfectly', 'RB')]\n"]}]}]}