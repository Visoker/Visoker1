{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled3.ipynb","provenance":[],"authorship_tag":"ABX9TyMxEpKGoMVQtXSFTLA6k5hz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"TRtT1J9TpU94"},"outputs":[],"source":[""]},{"cell_type":"markdown","source":["# Importing the necessary libraries\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import warnings\n","warnings.filterwarnings('ignore')\n","%matplotlib inline\n","Data\n","Importing Raw data\n","# Importing the csv file\n","data = pd.read_excel('INX_Future_Inc_Employee_Performance_CDS_Project2_Data_V1.8.xls')\n","Source Code\n","Exploratory Data Analysis\n","data.shape\n","(1200, 28)\n","data.columns\n","Index(['EmpNumber', 'Age', 'Gender', 'EducationBackground', 'MaritalStatus',\n","       'EmpDepartment', 'EmpJobRole', 'BusinessTravelFrequency',\n","       'DistanceFromHome', 'EmpEducationLevel', 'EmpEnvironmentSatisfaction',\n","       'EmpHourlyRate', 'EmpJobInvolvement', 'EmpJobLevel',\n","       'EmpJobSatisfaction', 'NumCompaniesWorked', 'OverTime',\n","       'EmpLastSalaryHikePercent', 'EmpRelationshipSatisfaction',\n","       'TotalWorkExperienceInYears', 'TrainingTimesLastYear',\n","       'EmpWorkLifeBalance', 'ExperienceYearsAtThisCompany',\n","       'ExperienceYearsInCurrentRole', 'YearsSinceLastPromotion',\n","       'YearsWithCurrManager', 'Attrition', 'PerformanceRating'],\n","      dtype='object')\n","data.head()\n","EmpNumber\tAge\tGender\tEducationBackground\tMaritalStatus\tEmpDepartment\tEmpJobRole\tBusinessTravelFrequency\tDistanceFromHome\tEmpEducationLevel\t...\tEmpRelationshipSatisfaction\tTotalWorkExperienceInYears\tTrainingTimesLastYear\tEmpWorkLifeBalance\tExperienceYearsAtThisCompany\tExperienceYearsInCurrentRole\tYearsSinceLastPromotion\tYearsWithCurrManager\tAttrition\tPerformanceRating\n","0\tE1001000\t32\tMale\tMarketing\tSingle\tSales\tSales Executive\tTravel_Rarely\t10\t3\t...\t4\t10\t2\t2\t10\t7\t0\t8\tNo\t3\n","1\tE1001006\t47\tMale\tMarketing\tSingle\tSales\tSales Executive\tTravel_Rarely\t14\t4\t...\t4\t20\t2\t3\t7\t7\t1\t7\tNo\t3\n","2\tE1001007\t40\tMale\tLife Sciences\tMarried\tSales\tSales Executive\tTravel_Frequently\t5\t4\t...\t3\t20\t2\t3\t18\t13\t1\t12\tNo\t4\n","3\tE1001009\t41\tMale\tHuman Resources\tDivorced\tHuman Resources\tManager\tTravel_Rarely\t10\t4\t...\t2\t23\t2\t2\t21\t6\t12\t6\tNo\t3\n","4\tE1001010\t60\tMale\tMarketing\tSingle\tSales\tSales Executive\tTravel_Rarely\t16\t4\t...\t4\t10\t1\t3\t2\t2\t2\t2\tNo\t3\n","5 rows × 28 columns\n","\n","# Looking for missing data\n","data.info()\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1200 entries, 0 to 1199\n","Data columns (total 28 columns):\n","EmpNumber                       1200 non-null object\n","Age                             1200 non-null int64\n","Gender                          1200 non-null object\n","EducationBackground             1200 non-null object\n","MaritalStatus                   1200 non-null object\n","EmpDepartment                   1200 non-null object\n","EmpJobRole                      1200 non-null object\n","BusinessTravelFrequency         1200 non-null object\n","DistanceFromHome                1200 non-null int64\n","EmpEducationLevel               1200 non-null int64\n","EmpEnvironmentSatisfaction      1200 non-null int64\n","EmpHourlyRate                   1200 non-null int64\n","EmpJobInvolvement               1200 non-null int64\n","EmpJobLevel                     1200 non-null int64\n","EmpJobSatisfaction              1200 non-null int64\n","NumCompaniesWorked              1200 non-null int64\n","OverTime                        1200 non-null object\n","EmpLastSalaryHikePercent        1200 non-null int64\n","EmpRelationshipSatisfaction     1200 non-null int64\n","TotalWorkExperienceInYears      1200 non-null int64\n","TrainingTimesLastYear           1200 non-null int64\n","EmpWorkLifeBalance              1200 non-null int64\n","ExperienceYearsAtThisCompany    1200 non-null int64\n","ExperienceYearsInCurrentRole    1200 non-null int64\n","YearsSinceLastPromotion         1200 non-null int64\n","YearsWithCurrManager            1200 non-null int64\n","Attrition                       1200 non-null object\n","PerformanceRating               1200 non-null int64\n","dtypes: int64(19), object(9)\n","memory usage: 262.6+ KB\n","Analysis of Department wise Perfomance\n","# A new pandas Dataframe is created to analyze department wise performance as asked.\n","dept = data.iloc[:,[5,27]].copy()\n","dept_per = dept.copy()\n","# Finding out the mean performance of all the departments and plotting its bar graph using seaborn.\n","dept_per.groupby(by='EmpDepartment')['PerformanceRating'].mean()\n","EmpDepartment\n","Data Science              3.050000\n","Development               3.085873\n","Finance                   2.775510\n","Human Resources           2.925926\n","Research & Development    2.921283\n","Sales                     2.860590\n","Name: PerformanceRating, dtype: float64\n","plt.figure(figsize=(10,4.5))\n","sns.barplot(dept_per['EmpDepartment'],dept_per['PerformanceRating'])\n","<matplotlib.axes._subplots.AxesSubplot at 0x286ab7ed748>\n","\n","# Analyze each department separately\n","dept_per.groupby(by='EmpDepartment')['PerformanceRating'].value_counts()\n","EmpDepartment           PerformanceRating\n","Data Science            3                     17\n","                        4                      2\n","                        2                      1\n","Development             3                    304\n","                        4                     44\n","                        2                     13\n","Finance                 3                     30\n","                        2                     15\n","                        4                      4\n","Human Resources         3                     38\n","                        2                     10\n","                        4                      6\n","Research & Development  3                    234\n","                        2                     68\n","                        4                     41\n","Sales                   3                    251\n","                        2                     87\n","                        4                     35\n","Name: PerformanceRating, dtype: int64\n","# Creating a new dataframe to analyze each department separately\n","department = pd.get_dummies(dept_per['EmpDepartment'])\n","performance = pd.DataFrame(dept_per['PerformanceRating'])\n","dept_rating = pd.concat([department,performance],axis=1)\n","# Plotting a separate bar graph for performance of each department using seaborn\n","plt.figure(figsize=(15,10))\n","plt.subplot(2,3,1)\n","sns.barplot(dept_rating['PerformanceRating'],dept_rating['Sales'])\n","plt.subplot(2,3,2)\n","sns.barplot(dept_rating['PerformanceRating'],dept_rating['Development'])\n","plt.subplot(2,3,3)\n","sns.barplot(dept_rating['PerformanceRating'],dept_rating['Research & Development'])\n","plt.subplot(2,3,4)\n","sns.barplot(dept_rating['PerformanceRating'],dept_rating['Human Resources'])\n","plt.subplot(2,3,5)\n","sns.barplot(dept_rating['PerformanceRating'],dept_rating['Finance'])\n","plt.subplot(2,3,6)\n","sns.barplot(dept_rating['PerformanceRating'],dept_rating['Data Science'])\n","plt.show()\n","\n","Data Processing/ Data Munging\n","# Encoding all the ordinal columns and creating a dummy variable for them to see if there are any effects on Performance Rating\n","enc = LabelEncoder()\n","for i in (2,3,4,5,6,7,16,26):\n","    data.iloc[:,i] = enc.fit_transform(data.iloc[:,i])\n","data.head()\n","EmpNumber\tAge\tGender\tEducationBackground\tMaritalStatus\tEmpDepartment\tEmpJobRole\tBusinessTravelFrequency\tDistanceFromHome\tEmpEducationLevel\t...\tEmpRelationshipSatisfaction\tTotalWorkExperienceInYears\tTrainingTimesLastYear\tEmpWorkLifeBalance\tExperienceYearsAtThisCompany\tExperienceYearsInCurrentRole\tYearsSinceLastPromotion\tYearsWithCurrManager\tAttrition\tPerformanceRating\n","0\tE1001000\t32\t1\t2\t2\t5\t13\t2\t10\t3\t...\t4\t10\t2\t2\t10\t7\t0\t8\t0\t3\n","1\tE1001006\t47\t1\t2\t2\t5\t13\t2\t14\t4\t...\t4\t20\t2\t3\t7\t7\t1\t7\t0\t3\n","2\tE1001007\t40\t1\t1\t1\t5\t13\t1\t5\t4\t...\t3\t20\t2\t3\t18\t13\t1\t12\t0\t4\n","3\tE1001009\t41\t1\t0\t0\t3\t8\t2\t10\t4\t...\t2\t23\t2\t2\t21\t6\t12\t6\t0\t3\n","4\tE1001010\t60\t1\t2\t2\t5\t13\t2\t16\t4\t...\t4\t10\t1\t3\t2\t2\t2\t2\t0\t3\n","5 rows × 28 columns\n","\n","Feature Selection\n","There are a lot of columns in the predictor variable. So, the correlation coeffecient is calculated to see which of them are important and these are then used for training methods. From there, we also get the top factors which affect performance. We can see that the most important features selectd were Department, Job Role, Environment Satisfaction, Last Salary Hike Percent, Work Life Balance, Experience Years At This Company, Experience Years In Current Role, Years Since Last Promotion, Years With Current Manager. These were selected because their correlation coeffecient with Performance Rating was more than 0.1.\n","Standardization and Label Encoding was also used for feature transformation.\n","A separate analysis considering all the predictors was carried out but it resulted in decreasing the accuracy. Similarly, Principal Component Analysis also reduces the accuracy.\n","Top 3 factors which affect the employee performance are 1. Employee EnvironmentSatisfaction, 2. Employee Last Salary Hike Percent and 3. Years Since Last Promotion\n","# Finding out the correlation coeffecient to find out which predictors are significant.\n","data.corr()\n","Age\tGender\tEducationBackground\tMaritalStatus\tEmpDepartment\tEmpJobRole\tBusinessTravelFrequency\tDistanceFromHome\tEmpEducationLevel\tEmpEnvironmentSatisfaction\t...\tEmpRelationshipSatisfaction\tTotalWorkExperienceInYears\tTrainingTimesLastYear\tEmpWorkLifeBalance\tExperienceYearsAtThisCompany\tExperienceYearsInCurrentRole\tYearsSinceLastPromotion\tYearsWithCurrManager\tAttrition\tPerformanceRating\n","Age\t1.000000\t-0.040107\t-0.055905\t-0.098368\t-0.000104\t-0.037665\t0.040579\t0.020937\t0.207313\t0.013814\t...\t0.049749\t0.680886\t-0.016053\t-0.019563\t0.318852\t0.217163\t0.228199\t0.205098\t-0.189317\t-0.040164\n","Gender\t-0.040107\t1.000000\t0.009922\t-0.042169\t-0.010925\t0.011332\t-0.043608\t-0.001507\t-0.022960\t0.000033\t...\t0.030707\t-0.061055\t-0.057654\t0.015793\t-0.030392\t-0.031823\t-0.021575\t-0.036643\t0.035758\t-0.001780\n","EducationBackground\t-0.055905\t0.009922\t1.000000\t-0.001097\t-0.026874\t-0.012325\t0.012382\t-0.013919\t-0.047978\t0.045028\t...\t0.005652\t-0.027929\t0.051596\t0.022890\t-0.009887\t-0.003215\t0.014277\t0.002767\t0.027161\t0.005607\n","MaritalStatus\t-0.098368\t-0.042169\t-0.001097\t1.000000\t0.067272\t0.038023\t0.028520\t-0.019148\t0.026737\t-0.032467\t...\t0.026410\t-0.093537\t0.026045\t0.014154\t-0.075728\t-0.076663\t-0.052951\t-0.061908\t0.162969\t0.024172\n","EmpDepartment\t-0.000104\t-0.010925\t-0.026874\t0.067272\t1.000000\t0.568973\t-0.045233\t0.007707\t0.019175\t-0.019237\t...\t-0.050286\t0.016065\t0.016438\t0.068875\t0.047677\t0.069602\t0.052315\t0.033850\t0.048006\t-0.162615\n","EmpJobRole\t-0.037665\t0.011332\t-0.012325\t0.038023\t0.568973\t1.000000\t-0.086251\t0.022939\t-0.016792\t0.044612\t...\t-0.043067\t-0.049529\t0.004452\t-0.007519\t-0.009047\t0.019383\t0.012190\t-0.004504\t0.037508\t-0.096209\n","BusinessTravelFrequency\t0.040579\t-0.043608\t0.012382\t0.028520\t-0.045233\t-0.086251\t1.000000\t-0.020935\t0.002064\t0.012267\t...\t-0.032705\t0.042736\t0.006720\t-0.040969\t-0.015029\t-0.006541\t-0.020824\t-0.028073\t0.007217\t-0.031025\n","DistanceFromHome\t0.020937\t-0.001507\t-0.013919\t-0.019148\t0.007707\t0.022939\t-0.020935\t1.000000\t0.045856\t-0.017719\t...\t-0.009509\t0.027306\t-0.032082\t-0.044788\t0.021908\t0.019898\t0.013246\t0.017860\t0.063248\t-0.046142\n","EmpEducationLevel\t0.207313\t-0.022960\t-0.047978\t0.026737\t0.019175\t-0.016792\t0.002064\t0.045856\t1.000000\t-0.037103\t...\t-0.016690\t0.151062\t-0.013674\t0.010276\t0.076332\t0.066672\t0.054313\t0.088988\t-0.049118\t0.020529\n","EmpEnvironmentSatisfaction\t0.013814\t0.000033\t0.045028\t-0.032467\t-0.019237\t0.044612\t0.012267\t-0.017719\t-0.037103\t1.000000\t...\t-0.010504\t-0.012894\t0.001192\t-0.000262\t-0.000561\t0.025491\t0.010732\t-0.011702\t-0.123490\t0.395561\n","EmpHourlyRate\t0.062867\t0.002218\t-0.030234\t-0.013540\t0.003957\t-0.016179\t0.025400\t0.013730\t0.014095\t-0.049501\t...\t0.008783\t0.026034\t-0.024160\t0.016189\t-0.000399\t-0.011871\t-0.010000\t-0.004576\t-0.026313\t-0.043116\n","EmpJobInvolvement\t0.027216\t0.010949\t-0.025505\t-0.043355\t-0.076988\t-0.008034\t0.016652\t0.003231\t0.027544\t0.004865\t...\t0.018037\t-0.028851\t-0.025168\t-0.014129\t-0.039720\t0.002910\t-0.019944\t0.012924\t-0.153360\t-0.010539\n","EmpJobLevel\t0.509139\t-0.050685\t-0.056338\t-0.087359\t0.100526\t0.004406\t0.036360\t0.017270\t0.100734\t-0.008272\t...\t0.002992\t0.784229\t-0.000389\t0.049218\t0.540377\t0.399235\t0.360880\t0.374872\t-0.175763\t-0.076632\n","EmpJobSatisfaction\t-0.002436\t0.024680\t-0.030977\t0.044593\t0.007150\t0.032916\t-0.031236\t-0.003036\t0.000357\t-0.004319\t...\t-0.022028\t-0.026824\t-0.028031\t-0.018548\t0.001807\t0.002018\t-0.006508\t-0.022096\t-0.081783\t0.000606\n","NumCompaniesWorked\t0.284408\t-0.036675\t-0.032879\t-0.030095\t-0.033950\t-0.009111\t0.021476\t-0.021411\t0.128674\t0.017270\t...\t0.057917\t0.221505\t-0.050817\t0.002489\t-0.129797\t-0.097271\t-0.031656\t-0.109937\t0.037643\t0.020980\n","OverTime\t0.051910\t-0.038410\t0.007046\t-0.022833\t-0.026841\t0.015075\t0.032229\t0.024940\t-0.021119\t0.064270\t...\t0.034146\t0.044233\t-0.061398\t-0.027968\t0.004295\t-0.015674\t0.000261\t-0.028447\t0.224535\t0.050206\n","EmpLastSalaryHikePercent\t-0.006105\t-0.005319\t-0.009788\t0.010128\t-0.012661\t0.005735\t-0.041946\t0.044974\t0.002358\t-0.047271\t...\t-0.042892\t-0.005933\t-0.013439\t-0.017001\t-0.019830\t-0.004957\t-0.015911\t-0.007666\t-0.009448\t0.333722\n","EmpRelationshipSatisfaction\t0.049749\t0.030707\t0.005652\t0.026410\t-0.050286\t-0.043067\t-0.032705\t-0.009509\t-0.016690\t-0.010504\t...\t1.000000\t0.018089\t0.025975\t-0.004906\t0.015612\t-0.032131\t0.026253\t-0.006426\t-0.037179\t-0.019502\n","TotalWorkExperienceInYears\t0.680886\t-0.061055\t-0.027929\t-0.093537\t0.016065\t-0.049529\t0.042736\t0.027306\t0.151062\t-0.012894\t...\t0.018089\t1.000000\t-0.022113\t0.015495\t0.633555\t0.460700\t0.412781\t0.460783\t-0.185226\t-0.068141\n","TrainingTimesLastYear\t-0.016053\t-0.057654\t0.051596\t0.026045\t0.016438\t0.004452\t0.006720\t-0.032082\t-0.013674\t0.001192\t...\t0.025975\t-0.022113\t1.000000\t0.037048\t0.008564\t0.010222\t0.018831\t-0.013237\t-0.035047\t-0.005443\n","EmpWorkLifeBalance\t-0.019563\t0.015793\t0.022890\t0.014154\t0.068875\t-0.007519\t-0.040969\t-0.044788\t0.010276\t-0.000262\t...\t-0.004906\t0.015495\t0.037048\t1.000000\t0.023479\t0.045719\t0.015773\t0.010518\t-0.068624\t0.124429\n","ExperienceYearsAtThisCompany\t0.318852\t-0.030392\t-0.009887\t-0.075728\t0.047677\t-0.009047\t-0.015029\t0.021908\t0.076332\t-0.000561\t...\t0.015612\t0.633555\t0.008564\t0.023479\t1.000000\t0.764102\t0.620230\t0.759258\t-0.142456\t-0.111645\n","ExperienceYearsInCurrentRole\t0.217163\t-0.031823\t-0.003215\t-0.076663\t0.069602\t0.019383\t-0.006541\t0.019898\t0.066672\t0.025491\t...\t-0.032131\t0.460700\t0.010222\t0.045719\t0.764102\t1.000000\t0.540600\t0.728973\t-0.173895\t-0.147638\n","YearsSinceLastPromotion\t0.228199\t-0.021575\t0.014277\t-0.052951\t0.052315\t0.012190\t-0.020824\t0.013246\t0.054313\t0.010732\t...\t0.026253\t0.412781\t0.018831\t0.015773\t0.620230\t0.540600\t1.000000\t0.491199\t-0.057199\t-0.167629\n","YearsWithCurrManager\t0.205098\t-0.036643\t0.002767\t-0.061908\t0.033850\t-0.004504\t-0.028073\t0.017860\t0.088988\t-0.011702\t...\t-0.006426\t0.460783\t-0.013237\t0.010518\t0.759258\t0.728973\t0.491199\t1.000000\t-0.158745\t-0.122313\n","Attrition\t-0.189317\t0.035758\t0.027161\t0.162969\t0.048006\t0.037508\t0.007217\t0.063248\t-0.049118\t-0.123490\t...\t-0.037179\t-0.185226\t-0.035047\t-0.068624\t-0.142456\t-0.173895\t-0.057199\t-0.158745\t1.000000\t-0.039796\n","PerformanceRating\t-0.040164\t-0.001780\t0.005607\t0.024172\t-0.162615\t-0.096209\t-0.031025\t-0.046142\t0.020529\t0.395561\t...\t-0.019502\t-0.068141\t-0.005443\t0.124429\t-0.111645\t-0.147638\t-0.167629\t-0.122313\t-0.039796\t1.000000\n","27 rows × 27 columns\n","\n","# Dropping the first columns as it is of no use for analysis.\n","data.drop(['EmpNumber'],inplace=True,axis=1)\n","data.head()\n","Age\tGender\tEducationBackground\tMaritalStatus\tEmpDepartment\tEmpJobRole\tBusinessTravelFrequency\tDistanceFromHome\tEmpEducationLevel\tEmpEnvironmentSatisfaction\t...\tEmpRelationshipSatisfaction\tTotalWorkExperienceInYears\tTrainingTimesLastYear\tEmpWorkLifeBalance\tExperienceYearsAtThisCompany\tExperienceYearsInCurrentRole\tYearsSinceLastPromotion\tYearsWithCurrManager\tAttrition\tPerformanceRating\n","0\t32\t1\t2\t2\t5\t13\t2\t10\t3\t4\t...\t4\t10\t2\t2\t10\t7\t0\t8\t0\t3\n","1\t47\t1\t2\t2\t5\t13\t2\t14\t4\t4\t...\t4\t20\t2\t3\t7\t7\t1\t7\t0\t3\n","2\t40\t1\t1\t1\t5\t13\t1\t5\t4\t4\t...\t3\t20\t2\t3\t18\t13\t1\t12\t0\t4\n","3\t41\t1\t0\t0\t3\t8\t2\t10\t4\t2\t...\t2\t23\t2\t2\t21\t6\t12\t6\t0\t3\n","4\t60\t1\t2\t2\t5\t13\t2\t16\t4\t1\t...\t4\t10\t1\t3\t2\t2\t2\t2\t0\t3\n","5 rows × 27 columns\n","\n","# Here we have selected only the important columns\n","y = data.PerformanceRating\n","#X = data.iloc[:,0:-1]  All predictors were selected it resulted in dropping of accuracy.\n","X = data.iloc[:,[4,5,9,16,20,21,22,23,24]] # Taking only variables with correlation coeffecient greater than 0.1\n","X.head()\n","EmpDepartment\tEmpJobRole\tEmpEnvironmentSatisfaction\tEmpLastSalaryHikePercent\tEmpWorkLifeBalance\tExperienceYearsAtThisCompany\tExperienceYearsInCurrentRole\tYearsSinceLastPromotion\tYearsWithCurrManager\n","0\t5\t13\t4\t12\t2\t10\t7\t0\t8\n","1\t5\t13\t4\t12\t3\t7\t7\t1\t7\n","2\t5\t13\t4\t21\t3\t18\t13\t1\t12\n","3\t3\t8\t2\t15\t2\t21\t6\t12\t6\n","4\t5\t13\t1\t14\t3\t2\t2\t2\t2\n","# Splitting into train and test for calculating the accuracy\n","X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=10)\n","# Standardization technique is used\n","sc = StandardScaler()\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test)\n","X_train.shape\n","(840, 9)\n","X_test.shape\n","(360, 9)\n","PCA was used, it resulted in decline of accuracy.\n","from sklearn.decomposition import PCA\n","pca=PCA(n_components=None)\n","X_train=pca.fit_transform(X_train)\n","X_test=pca.transform(X_test)\n","pca.explained_variance_ratio_\n","Models\n","In the section below, we used algorithms like Logistic Regression, Support Vector Machine, Decision Tree, Random Forest, Naive Bayes, K-Nearest Neighbor, XGBoost Classifier and Artificial Neural Network to calculate the accuracy and found out that Random Forest with GridSearchCV gives the maximum accuracy of 93%.\n","1. Logistic Regression\n","# Training the model\n","from sklearn.linear_model import LogisticRegression\n","model_logr = LogisticRegression()\n","model_logr.fit(X_train,y_train)\n","LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n","          intercept_scaling=1, max_iter=100, multi_class='warn',\n","          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n","          tol=0.0001, verbose=0, warm_start=False)\n","# Predicting the model\n","y_predict_log = model_logr.predict(X_test)\n","# Finding accuracy, precision, recall and confusion matrix\n","print(accuracy_score(y_test,y_predict_log))\n","print(classification_report(y_test,y_predict_log))\n","0.8194444444444444\n","              precision    recall  f1-score   support\n","\n","           2       0.68      0.40      0.50        63\n","           3       0.84      0.96      0.90       264\n","           4       0.77      0.52      0.62        33\n","\n","   micro avg       0.82      0.82      0.82       360\n","   macro avg       0.76      0.62      0.67       360\n","weighted avg       0.81      0.82      0.80       360\n","\n","confusion_matrix(y_test,y_predict_log)\n","array([[ 25,  35,   3],\n","       [  9, 253,   2],\n","       [  3,  13,  17]], dtype=int64)\n","2. Support Vector Machine\n","# Training the model\n","from sklearn.svm import SVC\n","rbf_svc = SVC(kernel='rbf', C=100, random_state=10).fit(X_train,y_train)\n","# Predicting the model\n","y_predict_svm = rbf_svc.predict(X_test)\n","# Finding accuracy, precision, recall and confusion matrix\n","print(accuracy_score(y_test,y_predict_svm))\n","print(classification_report(y_test,y_predict_svm))\n","0.8444444444444444\n","              precision    recall  f1-score   support\n","\n","           2       0.74      0.78      0.76        63\n","           3       0.92      0.88      0.90       264\n","           4       0.54      0.67      0.59        33\n","\n","   micro avg       0.84      0.84      0.84       360\n","   macro avg       0.73      0.78      0.75       360\n","weighted avg       0.85      0.84      0.85       360\n","\n","confusion_matrix(y_test,y_predict_svm)\n","array([[ 49,  13,   1],\n","       [ 13, 233,  18],\n","       [  4,   7,  22]], dtype=int64)\n","3. Decision Tree with GridSearchCV\n","# Training the model\n","from sklearn.tree import DecisionTreeClassifier\n","\n","classifier_dtg=DecisionTreeClassifier(random_state=42,splitter='best')\n","parameters=[{'min_samples_split':[2,3,4,5],'criterion':['gini']},{'min_samples_split':[2,3,4,5],'criterion':['entropy']}]\n","\n","model_griddtree=GridSearchCV(estimator=classifier_dtg, param_grid=parameters, scoring='accuracy',cv=10)\n","model_griddtree.fit(X_train,y_train)\n","GridSearchCV(cv=10, error_score='raise-deprecating',\n","       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n","            max_features=None, max_leaf_nodes=None,\n","            min_impurity_decrease=0.0, min_impurity_split=None,\n","            min_samples_leaf=1, min_samples_split=2,\n","            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n","            splitter='best'),\n","       fit_params=None, iid='warn', n_jobs=None,\n","       param_grid=[{'min_samples_split': [2, 3, 4, 5], 'criterion': ['gini']}, {'min_samples_split': [2, 3, 4, 5], 'criterion': ['entropy']}],\n","       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n","       scoring='accuracy', verbose=0)\n","model_griddtree.best_params_\n","{'criterion': 'entropy', 'min_samples_split': 5}\n","# Predicting the model\n","y_predict_dtree = model_griddtree.predict(X_test)\n","# Finding accuracy, precision, recall and confusion matrix\n","print(accuracy_score(y_test,y_predict_dtree))\n","print(classification_report(y_test,y_predict_dtree))\n","0.9055555555555556\n","              precision    recall  f1-score   support\n","\n","           2       0.85      0.83      0.84        63\n","           3       0.94      0.95      0.94       264\n","           4       0.75      0.73      0.74        33\n","\n","   micro avg       0.91      0.91      0.91       360\n","   macro avg       0.85      0.83      0.84       360\n","weighted avg       0.90      0.91      0.90       360\n","\n","confusion_matrix(y_test,y_predict_dtree)\n","array([[ 52,  10,   1],\n","       [  7, 250,   7],\n","       [  2,   7,  24]], dtype=int64)\n","4. Random Forest with GridSearchCV\n","# Training the model\n","from sklearn.ensemble import RandomForestClassifier\n","\n","classifier_rfg=RandomForestClassifier(random_state=33,n_estimators=23)\n","parameters=[{'min_samples_split':[2,3,4,5],'criterion':['gini','entropy'],'min_samples_leaf':[1,2,3]}]\n","\n","model_gridrf=GridSearchCV(estimator=classifier_rfg, param_grid=parameters, scoring='accuracy',cv=10)\n","model_gridrf.fit(X_train,y_train)\n","GridSearchCV(cv=10, error_score='raise-deprecating',\n","       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n","            max_depth=None, max_features='auto', max_leaf_nodes=None,\n","            min_impurity_decrease=0.0, min_impurity_split=None,\n","            min_samples_leaf=1, min_samples_split=2,\n","            min_weight_fraction_leaf=0.0, n_estimators=23, n_jobs=None,\n","            oob_score=False, random_state=33, verbose=0, warm_start=False),\n","       fit_params=None, iid='warn', n_jobs=None,\n","       param_grid=[{'min_samples_split': [2, 3, 4, 5], 'criterion': ['gini', 'entropy'], 'min_samples_leaf': [1, 2, 3]}],\n","       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n","       scoring='accuracy', verbose=0)\n","model_gridrf.best_params_\n","{'criterion': 'gini', 'min_samples_leaf': 1, 'min_samples_split': 4}\n","# Predicting the model\n","y_predict_rf = model_gridrf.predict(X_test)\n","# Finding accuracy, precision, recall and confusion matrix\n","print(accuracy_score(y_test,y_predict_rf))\n","print(classification_report(y_test,y_predict_rf))\n","0.9305555555555556\n","              precision    recall  f1-score   support\n","\n","           2       0.92      0.89      0.90        63\n","           3       0.94      0.97      0.96       264\n","           4       0.83      0.73      0.77        33\n","\n","   micro avg       0.93      0.93      0.93       360\n","   macro avg       0.90      0.86      0.88       360\n","weighted avg       0.93      0.93      0.93       360\n","\n","confusion_matrix(y_test,y_predict_rf)\n","array([[ 56,   7,   0],\n","       [  4, 255,   5],\n","       [  1,   8,  24]], dtype=int64)\n","5. Naive Bayes Bernoulli\n","# Training the model\n","from sklearn.naive_bayes import BernoulliNB\n","model_nb = BernoulliNB()\n","model_nb.fit(X_train,y_train)\n","BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n","# Predicting the model\n","y_predict_nb = model_nb.predict(X_test)\n","# Finding accuracy, precision, recall and confusion matrix\n","print(accuracy_score(y_test,y_predict_nb))\n","print(classification_report(y_test,y_predict_nb))\n","0.7944444444444444\n","              precision    recall  f1-score   support\n","\n","           2       0.75      0.48      0.58        63\n","           3       0.80      0.97      0.88       264\n","           4       0.00      0.00      0.00        33\n","\n","   micro avg       0.79      0.79      0.79       360\n","   macro avg       0.52      0.48      0.49       360\n","weighted avg       0.72      0.79      0.74       360\n","\n","confusion_matrix(y_test,y_predict_nb)\n","array([[ 30,  33,   0],\n","       [  8, 256,   0],\n","       [  2,  31,   0]], dtype=int64)\n","6. K-Nearest Neighbor\n","# Training the model\n","from sklearn.neighbors import KNeighborsClassifier\n","model_knn = KNeighborsClassifier(n_neighbors=10,metric='euclidean') # Maximum accuracy for n=10\n","model_knn.fit(X_train,y_train)\n","KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n","           metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n","           weights='uniform')\n","# Predicting the model\n","y_predict_knn = model_knn.predict(X_test)\n","# Finding accuracy, precision, recall and confusion matrix\n","print(accuracy_score(y_test,y_predict_knn))\n","print(classification_report(y_test,y_predict_knn))\n","0.8333333333333334\n","              precision    recall  f1-score   support\n","\n","           2       0.74      0.59      0.65        63\n","           3       0.85      0.94      0.90       264\n","           4       0.78      0.42      0.55        33\n","\n","   micro avg       0.83      0.83      0.83       360\n","   macro avg       0.79      0.65      0.70       360\n","weighted avg       0.83      0.83      0.82       360\n","\n","confusion_matrix(y_test,y_predict_knn)\n","array([[ 37,  25,   1],\n","       [ 12, 249,   3],\n","       [  1,  18,  14]], dtype=int64)\n","7. XGBoost Classifier\n","# Training the model\n","from xgboost import XGBClassifier\n","model_xgb = XGBClassifier()\n","model_xgb.fit(X_train,y_train)\n","XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n","       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n","       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n","       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n","       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n","       silent=True, subsample=1)\n","# Predicting the model\n","y_predict_xgb = model_xgb.predict(X_test)\n","# Finding accuracy, precision, recall and confusion matrix\n","print(accuracy_score(y_test,y_predict_xgb))\n","print(classification_report(y_test,y_predict_xgb))\n","0.9277777777777778\n","              precision    recall  f1-score   support\n","\n","           2       0.90      0.87      0.89        63\n","           3       0.94      0.96      0.95       264\n","           4       0.83      0.76      0.79        33\n","\n","   micro avg       0.93      0.93      0.93       360\n","   macro avg       0.89      0.86      0.88       360\n","weighted avg       0.93      0.93      0.93       360\n","\n","confusion_matrix(y_test,y_predict_xgb)\n","array([[ 55,   8,   0],\n","       [  5, 254,   5],\n","       [  1,   7,  25]], dtype=int64)\n","8. Artificial Neural Network\n","# Training the model\n","from sklearn.neural_network import MLPClassifier\n","model_mlp = MLPClassifier(hidden_layer_sizes=(100,100,100),batch_size=10,learning_rate_init=0.01,max_iter=2000,random_state=10)\n","model_mlp.fit(X_train,y_train)\n","MLPClassifier(activation='relu', alpha=0.0001, batch_size=10, beta_1=0.9,\n","       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n","       hidden_layer_sizes=(100, 100, 100), learning_rate='constant',\n","       learning_rate_init=0.01, max_iter=2000, momentum=0.9,\n","       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n","       random_state=10, shuffle=True, solver='adam', tol=0.0001,\n","       validation_fraction=0.1, verbose=False, warm_start=False)\n","# Predicting the model\n","y_predict_mlp = model_mlp.predict(X_test)\n","# Finding accuracy, precision, recall and confusion matrix\n","print(accuracy_score(y_test,y_predict_mlp))\n","print(classification_report(y_test,y_predict_mlp))\n","0.9111111111111111\n","              precision    recall  f1-score   support\n","\n","           2       0.84      0.83      0.83        63\n","           3       0.94      0.95      0.95       264\n","           4       0.83      0.73      0.77        33\n","\n","   micro avg       0.91      0.91      0.91       360\n","   macro avg       0.87      0.84      0.85       360\n","weighted avg       0.91      0.91      0.91       360\n","\n","confusion_matrix(y_test,y_predict_mlp)\n","array([[ 52,  10,   1],\n","       [  8, 252,   4],\n","       [  2,   7,  24]], dtype=int64)\n","# Exporting the trained model\n","from sklearn.externals import joblib\n","joblib.dump(model_gridrf,'INX_Future_Inc.ml')\n","['INX_Future_Inc.ml']"],"metadata":{"id":"-TRyWxbMpZEc"}}]}